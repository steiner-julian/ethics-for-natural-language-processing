{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvM8edaAiOzT"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu3A3CtkIrxi"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aHIbn9sZKxs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QptDPUD5LKAM"
      },
      "outputs": [],
      "source": [
        "#Get Word2Vec Embeddings\n",
        "w2v = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEpzYIO0TCtz"
      },
      "outputs": [],
      "source": [
        "#Dictionary containing word as key and their index in w2v as value\n",
        "key_dict = w2v.key_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEjMuwjFjuyt"
      },
      "outputs": [],
      "source": [
        "#List of the words in the same order as w2v\n",
        "words = list(key_dict.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np87BdamjvXp"
      },
      "outputs": [],
      "source": [
        "key_dict['football']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhjwy52ijxgH"
      },
      "outputs": [],
      "source": [
        "words[723]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6C2bZO2hCUz"
      },
      "source": [
        "## Task 1 Word Vector Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcwdcHQOUt6C"
      },
      "outputs": [],
      "source": [
        "#Insert the embeddings of the first 10000 words from w2v into vec_list\n",
        "vec_list = []\n",
        "#your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOHc_e_BYwFV"
      },
      "outputs": [],
      "source": [
        "vec_array = np.array(vec_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9YT__HDZUvY"
      },
      "outputs": [],
      "source": [
        "vec_array.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ3jz2IUZOuQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-SYcPSpZXLo"
      },
      "outputs": [],
      "source": [
        "#Use TSNE to reduce the dimension of vectors in vec_array from 300 to 2 and produce a reduced vector matrix. Print its shape for checking.\n",
        "#your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo1oCGLwaq2p"
      },
      "outputs": [],
      "source": [
        "#Lists for plotting them on graph\n",
        "labels = []\n",
        "dim1 = []\n",
        "dim2 = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjezx0tiZ_z4"
      },
      "outputs": [],
      "source": [
        "#Put the words in list ['football','tennis','hockey','baseball','field','court','law','science','literature','computer','games'] in labels.\n",
        "#Put the first dimension from the reduced vectors corresponding to the words in dim1 list\n",
        "#Put the second dimension from the reduced vectors corresponding to the words in dim2 list\n",
        "#your code here\n",
        "for word in ['football','tennis','hockey','baseball','field','court','law','science','literature','computer','games']:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfYQujAnd4E4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for i in range(len(labels)):\n",
        "  plt.plot(dim1[i],dim2[i],'o',label=labels[i])\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-edZhJvg7_n"
      },
      "source": [
        "## Task 2.1 Simple Bias Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfEG5PpgYRCL"
      },
      "outputs": [],
      "source": [
        "#Use the most_similar function in w2v to print words most similar to \"job\" and \"men\" and most dissimilar to \"women\"\n",
        "#your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3EDj-oRYVPc"
      },
      "outputs": [],
      "source": [
        "#Use the most_similar function in w2v to print words most similar to \"job\" and \"women\" and most dissimilar to \"men\"\n",
        "#your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnFNtQZdgxET"
      },
      "source": [
        "## Task 2.2 Bias Analysis in models trained with word2vec embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwpADnSxitul"
      },
      "outputs": [],
      "source": [
        "#Get SemEval 2018 Train data\n",
        "!wget http://saifmohammad.com/WebDocs/AIT-2018/AIT2018-DATA/V-reg/English/2018-Valence-reg-En-train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COLlop-kGDYT"
      },
      "outputs": [],
      "source": [
        "#Get SemEval 2018 Test Data\n",
        "!wget http://saifmohammad.com/WebDocs/SemEval2018-Task1-AIT-Test-gold.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOQQA48oGHaM"
      },
      "outputs": [],
      "source": [
        "!unzip 2018-Valence-reg-En-train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owzxtc4kGZqO"
      },
      "outputs": [],
      "source": [
        "!unzip SemEval2018-Task1-AIT-Test-gold.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi_JcfAkGMoy"
      },
      "outputs": [],
      "source": [
        "#Load train dataframe in train_df\n",
        "train_df = pd.read_csv('2018-Valence-reg-En-train.txt',\n",
        "                       sep='\\t', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hRkq3G4Gcrl"
      },
      "outputs": [],
      "source": [
        "#Load test dataframe in test_df\n",
        "test_df = pd.read_csv('SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt',\n",
        "                       sep='\\t', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8W1S3X7GgQn"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEli6MLaGpef"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEQHoEjOGjY5"
      },
      "outputs": [],
      "source": [
        "# Convert all the intensity score from real numbers into boolean values,\n",
        "# setting the threshold at 0.5, and creating a new column named\n",
        "# `label`\n",
        "# label 1 if greater than thershold\n",
        "# label 0 if less than or equal to threshold\n",
        "#your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQmuyw5ZIYWd",
        "outputId": "eeba0880-017a-4b97-ab55-55d34e22625e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['This', 'short', 'text']"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.parsing.preprocessing import (preprocess_string,\n",
        "                                          strip_tags,\n",
        "                                          strip_punctuation,\n",
        "                                          strip_multiple_whitespaces,\n",
        "                                          strip_numeric,\n",
        "                                          remove_stopwords)\n",
        "\n",
        "\n",
        "# We pick a subset of the default filters,\n",
        "# in particular, we do not take\n",
        "# strip_short() and stem_text().\n",
        "FILTERS = [strip_punctuation,\n",
        "           strip_tags,\n",
        "           strip_multiple_whitespaces,\n",
        "           strip_numeric,\n",
        "           remove_stopwords]\n",
        "\n",
        "# See how the sentece is transformed into tokes (words)\n",
        "preprocess_string('This is a \"short\" text!', FILTERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbRsYYKTJ1Gt"
      },
      "outputs": [],
      "source": [
        "train_text_list = train_df[\"Tweet\"].tolist()\n",
        "train_label_list = train_df[\"label\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8w6t9XGJYW6"
      },
      "outputs": [],
      "source": [
        "train_text = []\n",
        "train_label = []\n",
        "#Make (input,output) pairs for training\n",
        "for i in range(len(train_text_list)):\n",
        "  if len(train_text_list[i])!=0:\n",
        "    #Get the preprocessed and tokenised text\n",
        "    processed_txt =\n",
        "    #Final vector\n",
        "    X_tensor = np.zeros(300)\n",
        "    #To keep track of number of words from the sentence in w2v\n",
        "    l = 0\n",
        "    #For each token in preprocessed text get the w2v vector, add it to X_tensor and finally average it by all tokens with w2v vectors. Basically do meanpooling.\n",
        "    #Append the input to train_text\n",
        "    #Append the labels to train_label\n",
        "    #Things to be cautious of: Words out of vocabulary for w2v and empty sentences in the training set(this are not to be considered)\n",
        "    #Also take care to check if l=0. Do not divide by l in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_hMujyrKVGW"
      },
      "outputs": [],
      "source": [
        "test_text_list = test_df[\"Tweet\"].tolist()\n",
        "test_label_list = test_df[\"label\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltaNjYS5Lath"
      },
      "outputs": [],
      "source": [
        "#Make (input,output) pairs for test set similar to train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbaDmF1DLq9z"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFzd9CcPMGtC"
      },
      "outputs": [],
      "source": [
        "#Train the logistic Regresion Model using Training Data\n",
        "#your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLZgMv72MOVh"
      },
      "outputs": [],
      "source": [
        "#Report accuracy in traning data\n",
        "#your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIWRlrBrMdN7"
      },
      "outputs": [],
      "source": [
        "#Report accuracy in testing data\n",
        "#your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6RpAQ8pMhxM"
      },
      "outputs": [],
      "source": [
        "#Now load the EEC dataset for testing gender bias\n",
        "eec_df = pd.read_csv('Equity-Evaluation-Corpus.csv')\n",
        "\n",
        "#Remove the sentences for evaluating racial bias\n",
        "gender_eec_df = eec_df[eec_df['Race'].isna()][:]\n",
        "\n",
        "#Create identifier to mach sentence pairs\n",
        "#The EEC data comes without this matching\n",
        "MALE_PERSONS = ('he', 'this man', 'this boy', 'my brother', 'my son', 'my husband',\n",
        "                'my boyfriend', 'my father', 'my uncle', 'my dad', 'him')\n",
        "\n",
        "FEMALE_PERSONS = ('she', 'this woman', 'this girl', 'my sister', 'my daughter', 'my wife',\n",
        "                  'my girlfriend', 'my mother', 'my aunt', 'my mom', 'her')\n",
        "\n",
        "MALE_IDENTIFIER = dict(zip(MALE_PERSONS, FEMALE_PERSONS))\n",
        "FEMALE_IDENTIFIER = dict(zip(FEMALE_PERSONS, FEMALE_PERSONS))\n",
        "\n",
        "PERSON_MATCH_WORDS = {**MALE_IDENTIFIER,\n",
        "                      **FEMALE_IDENTIFIER}\n",
        "\n",
        "gender_eec_df['PersonIdentifier'] = gender_eec_df['Person'].map(PERSON_MATCH_WORDS)\n",
        "\n",
        "gender_eec_df = gender_eec_df.sort_values(['Gender', 'Template', 'Emotion word', 'PersonIdentifier'])\n",
        "\n",
        "gender_split_index = len(gender_eec_df) // 2\n",
        "\n",
        "# Create two DataFrames, one for\n",
        "female_eec_df = gender_eec_df[:gender_split_index].reset_index(False)\n",
        "male_eec_df = gender_eec_df[gender_split_index:].reset_index(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5eKG10FM9Le"
      },
      "outputs": [],
      "source": [
        "female_eec_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga8rkvVRM_dC"
      },
      "outputs": [],
      "source": [
        "male_eec_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ROzCDUmQmwM"
      },
      "outputs": [],
      "source": [
        "#Get a subset of the male subset which relates to the emotion 'joy'\n",
        "#your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xMeJ_a8Q5yp"
      },
      "outputs": [],
      "source": [
        "#Make a input matrix with sentences from the joy-male subset to feed to the trained logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoSQVoIPStfE"
      },
      "outputs": [],
      "source": [
        "#Make a histogram/distribution plot of the probability of getting valence label 1 for this subset\n",
        "#Use the predict_proba function of logistic regression and your trained logistic regression model from previous step for getting the probability\n",
        "#Use the distplot function from seaborn library for the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoZnq2aCTcID"
      },
      "outputs": [],
      "source": [
        "#Get a subset of the female subset which relates to the emotion 'joy'\n",
        "#your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r8Zlii3UDux"
      },
      "outputs": [],
      "source": [
        "#Make a input matrix with sentences from the joy-female subset to feed to the trained logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjRuHKGbUU9P"
      },
      "outputs": [],
      "source": [
        "#Make a histogram/distribution plot of the probability of getting valence label 1 for this subset\n",
        "#Use the predict_proba function of logistic regression and your trained logistic regression model from previous step for getting the probability\n",
        "#Use the distplot function from seaborn library for the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy5dmQUAUYih"
      },
      "outputs": [],
      "source": [
        "#Get both plots on the same graph with labels"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
